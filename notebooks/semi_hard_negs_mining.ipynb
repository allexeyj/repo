{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "datasets creation",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu-cu12 xformers torch torchvision transformers  sentence_transformers -U"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T19:04:57.921584Z",
          "iopub.execute_input": "2025-04-19T19:04:57.92187Z",
          "iopub.status.idle": "2025-04-19T19:08:06.9655Z",
          "shell.execute_reply.started": "2025-04-19T19:04:57.921846Z",
          "shell.execute_reply": "2025-04-19T19:08:06.964599Z"
        },
        "id": "lxi1GRGmqlo5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "–§–æ—Ä–º–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç triplets (q, pos, [neg‚ÇÅ ‚Ä¶ neg‚Çá])\n",
        "–¥–ª—è —Å–ø–ª–∏—Ç–æ–≤, –≥–¥–µ `positive` ‚Äî –æ–¥–∏–Ω–æ—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "\n",
        "import datasets\n",
        "import faiss\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Config\n",
        "# ==========================\n",
        "CONFIG = {\n",
        "    \"prefix_type\":          \"search\",\n",
        "    \"hard_neg_rank_start\":  10,\n",
        "    \"hard_neg_rank_end\":    50,\n",
        "    \"hard_neg_count\":       5,\n",
        "    \"max_triplets\":         100_000,\n",
        "    \"log_every\":            500,\n",
        "    \"max_tokens\":           1024,\n",
        "    \"sample_size\":          5_000,\n",
        "    \"margin_tau\":           0.01,\n",
        "\n",
        "    \"dataset_path\":         \"zloelias/lenta-ru\",\n",
        "    \"query_column\":         \"title\",\n",
        "    \"positive_column\":      \"text\",        # <-- –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞\n",
        "    \"train_split\":          \"train\",\n",
        "\n",
        "    \"hf_token\":             \"<token>\",\n",
        "    \"target_repo\":          \"Alexator26/lenta-ru-triplets\"\n",
        "}\n",
        "\n",
        "# ==========================\n",
        "# Model & tokenizer\n",
        "# ==========================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepvk/USER2-base\")\n",
        "model     = SentenceTransformer(\"deepvk/USER2-base\")\n",
        "\n",
        "\n",
        "# ========= helpers =========\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text, truncation=False))\n",
        "\n",
        "\n",
        "\n",
        "def prompt_name(is_query: bool) -> str: #–≤—Å–µ–≥–¥–∞ —é–∑–∞–ª–æ—Å—å search –ø—Ä–∏ –º–∞–π–Ω–∏–Ω–≥–µ\n",
        "    if CONFIG[\"prefix_type\"] == \"search\":\n",
        "        return \"search_query\" if is_query else \"search_document\"\n",
        "    return \"clustering\"\n",
        "\n",
        "\n",
        "def create_triplets(\n",
        "        split,\n",
        "        q_field: str,\n",
        "        p_field: str,\n",
        "        *,\n",
        "        max_tokens : int   = 500,\n",
        "        margin_tau : float = CONFIG['margin_tau']         # pos_sim ‚Äì neg_sim ‚â• œÑ (semi‚Äëhard)\n",
        "    ):\n",
        "    \"\"\"\n",
        "    –°—Ç—Ä–æ–∏—Ç —Ç—Ä–∏–ø–ª–µ—Ç—ã (query, positive, [neg‚Ä¶]) c semi‚Äëhard –Ω–µ–≥–∞—Ç–∏–≤–∞–º–∏:\n",
        "        sim(q, neg) < sim(q, pos) ‚Äì œÑ.\n",
        "\n",
        "    –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ CONFIG:\n",
        "        ‚Ä¢ hard_neg_rank_start / _end  ‚Äì –¥–∏–∞–ø–∞–∑–æ–Ω FAISS‚Äë—Ä–∞–Ω–≥–æ–≤,\n",
        "        ‚Ä¢ hard_neg_count              ‚Äì —Å–∫–æ–ª—å–∫–æ –Ω–µ–≥–∞—Ç–∏–≤–æ–≤ –≤ —Ç—Ä–∏–ø–ª–µ—Ç–µ.\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 0. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ\n",
        "    # ------------------------------------------------------------------ #\n",
        "    examples = [ex for ex in split if ex.get(q_field) and ex.get(p_field)]\n",
        "    random.shuffle(examples)\n",
        "    if not examples:\n",
        "        return []\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 1. –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º positive‚Äë–¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
        "    # ------------------------------------------------------------------ #\n",
        "    unique_docs, doc2idx, ex2doc_idx = [], {}, {}\n",
        "    for ex_id, ex in enumerate(tqdm(examples, desc=\"Collect uniques\")):\n",
        "        doc = ex[p_field]\n",
        "        if count_tokens(doc) > max_tokens:\n",
        "            continue\n",
        "\n",
        "        # –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑\n",
        "        if doc not in doc2idx:\n",
        "            doc_idx = len(unique_docs)\n",
        "            doc2idx[doc] = doc_idx\n",
        "            unique_docs.append(doc)\n",
        "\n",
        "        ex2doc_idx[ex_id] = doc2idx[doc]\n",
        "\n",
        "    if not unique_docs:\n",
        "        return []\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 2. FAISS‚Äë–∏–Ω–¥–µ–∫—Å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n",
        "    # ------------------------------------------------------------------ #\n",
        "    doc_embs = model.encode(\n",
        "        unique_docs,\n",
        "        prompt_name       = prompt_name(is_query=False),\n",
        "        convert_to_numpy  = True,\n",
        "        show_progress_bar = True\n",
        "    ).astype(\"float32\")\n",
        "\n",
        "    faiss.normalize_L2(doc_embs)                    # cosine == IP\n",
        "    index = faiss.IndexFlatIP(doc_embs.shape[1])\n",
        "    index.add(doc_embs)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 3. –ú–∞–π–Ω–Ω–∏–Ω–≥ —Å semi‚Äëhard –Ω–µ–≥–∞—Ç–∏–≤–∞–º–∏\n",
        "    # ------------------------------------------------------------------ #\n",
        "    triplets = []\n",
        "\n",
        "    for ex_id, ex in enumerate(tqdm(examples, desc=\"Mining triplets\")):\n",
        "        if len(triplets) >= CONFIG[\"max_triplets\"]:\n",
        "            break\n",
        "        if ex_id and ex_id % CONFIG[\"log_every\"] == 0:\n",
        "            print(f\"   {ex_id}/{len(examples)} ‚Üí {len(triplets)} triplets\")\n",
        "\n",
        "        q_text = ex[q_field]\n",
        "        if count_tokens(q_text) > max_tokens:\n",
        "            continue\n",
        "\n",
        "        pos_idx = ex2doc_idx.get(ex_id)\n",
        "        if pos_idx is None:\n",
        "            continue\n",
        "        pos_text = unique_docs[pos_idx]\n",
        "\n",
        "        # ---------- —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (1) ------------------------------\n",
        "        q_emb   = model.encode(\n",
        "            q_text,\n",
        "            prompt_name       = prompt_name(is_query=True),\n",
        "            convert_to_numpy  = True,\n",
        "            show_progress_bar = False\n",
        "        ).astype(\"float32\")                         # shape: (d,)\n",
        "\n",
        "        # –ø—Ä–∏–≤–æ–¥–∏–º –∫ shape (1, d) –¥–ª—è FAISS‚Äë–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "        q_emb_2d = q_emb.reshape(1, -1)\n",
        "        faiss.normalize_L2(q_emb_2d)               # in‚Äëplace\n",
        "        q_emb    = q_emb_2d[0]                     # —Å–Ω–æ–≤–∞ (d,)\n",
        "\n",
        "        # ---------- similarity —Å –ø–æ–∑–∏—Ç–∏–≤–æ–º -----------------------------\n",
        "        pos_sim = float(np.dot(q_emb, doc_embs[pos_idx]))\n",
        "\n",
        "        # ---------- –∏—â–µ–º —Å–æ—Å–µ–¥–µ–π ---------------------------------------\n",
        "        _, neigh = index.search(\n",
        "            q_emb_2d,                              # 2‚ÄëD !\n",
        "            CONFIG[\"hard_neg_rank_end\"] + 1\n",
        "        )\n",
        "\n",
        "        # ---------- semi‚Äëhard –æ—Ç–±–æ—Ä ------------------------------------\n",
        "        candidates = []\n",
        "        for i in neigh[0][CONFIG[\"hard_neg_rank_start\"]\n",
        "                          : CONFIG[\"hard_neg_rank_end\"] + 1]:\n",
        "            if i == pos_idx:                       # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤\n",
        "                continue\n",
        "\n",
        "            sim_i = float(np.dot(q_emb, doc_embs[i]))\n",
        "            if sim_i < pos_sim - margin_tau:       # semi‚Äëhard —É—Å–ª–æ–≤–∏–µ\n",
        "                candidates.append(i)\n",
        "\n",
        "        random.shuffle(candidates)\n",
        "        neg_idxs = candidates[:CONFIG[\"hard_neg_count\"]]\n",
        "\n",
        "        if len(neg_idxs) < CONFIG[\"hard_neg_count\"]:\n",
        "            continue\n",
        "\n",
        "        # ---------- —Å–∫–ª–∞–¥—ã–≤–∞–µ–º —Ç—Ä–∏–ø–ª–µ—Ç ---------------------------------\n",
        "        triplets.append({\n",
        "            \"query\"    : q_text,\n",
        "            \"positive\" : pos_text,\n",
        "            \"negatives\": [unique_docs[i]\n",
        "                          for i in neg_idxs]\n",
        "        })\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è\n",
        "    # ------------------------------------------------------------------ #\n",
        "    unique_triplets = {\n",
        "        (t[\"query\"], t[\"positive\"], tuple(t[\"negatives\"])): t\n",
        "        for t in triplets\n",
        "    }\n",
        "\n",
        "    return list(unique_triplets.values())\n",
        "\n",
        "# =========  quality check: are negatives really hard?  =========\n",
        "def evaluate_hard_negative_quality(triplets, sample_size=CONFIG['sample_size']):\n",
        "    \"\"\"\n",
        "    Quickly probes the mined triplets to be sure that\n",
        "    (a) positives are closer to the query than any negative\n",
        "    (b) negatives are still *close enough* to qualify as ¬´hard¬ª.\n",
        "\n",
        "    Returns a dict with interpretable metrics that can be dumped\n",
        "    straight into the dataset card (or just printed to console).\n",
        "    \"\"\"\n",
        "    # 1) ------------------  sampling  ------------------\n",
        "    sample = random.sample(triplets, k=min(sample_size, len(triplets)))\n",
        "\n",
        "    q_texts   = [t[\"query\"]               for t in sample]\n",
        "    pos_texts = [t[\"positive\"]            for t in sample]\n",
        "    neg_texts = [n for t in sample for n in t[\"negatives\"]]\n",
        "\n",
        "\n",
        "    # 2) ------------------  embeddings  ----------------\n",
        "    q_embs   = model.encode(q_texts,\n",
        "                            prompt_name=prompt_name(True),\n",
        "                            convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "    pos_embs = model.encode(pos_texts,\n",
        "                            prompt_name=prompt_name(False),\n",
        "                            convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "    neg_embs = model.encode(neg_texts,\n",
        "                            prompt_name=prompt_name(False),\n",
        "                            convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "    # cosine normalisation\n",
        "    q_embs   /= np.linalg.norm(q_embs,   axis=1, keepdims=True)\n",
        "    pos_embs /= np.linalg.norm(pos_embs, axis=1, keepdims=True)\n",
        "    neg_embs /= np.linalg.norm(neg_embs, axis=1, keepdims=True)\n",
        "\n",
        "    # 3) ------------------  similarity math -------------\n",
        "    pos_sims = (q_embs * pos_embs).sum(axis=1)\n",
        "\n",
        "    neg_sims, margins = [], []\n",
        "    idx, bad_cnt = 0, 0                                      # \"bad\" == negative ‚â• positive\n",
        "    for i, t in tqdm(enumerate(sample), desc='evaluate_hard_negative_quality', total=sample_size):\n",
        "        n = len(t[\"negatives\"])\n",
        "        sims = q_embs[i] @ neg_embs[idx: idx + n].T\n",
        "\n",
        "        hardest = sims.max()\n",
        "        neg_sims.extend(sims.tolist())\n",
        "\n",
        "        if hardest >= pos_sims[i]:\n",
        "            bad_cnt += 1\n",
        "        margins.append(pos_sims[i] - hardest)\n",
        "\n",
        "        idx += n\n",
        "\n",
        "    stats = {\n",
        "        \"sampled_triplets\":              len(sample),\n",
        "        \"mean_pos_sim\":                 float(np.mean(pos_sims)),\n",
        "        \"mean_neg_sim\":                 float(np.mean(neg_sims)),\n",
        "        \"mean_margin_pos_vs_hardest\":   float(np.mean(margins)),\n",
        "        \"triplets_with_harder_negative\": bad_cnt\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "\n",
        "def push_dataset(dsdict, readme_fragment):\n",
        "    api = HfApi()\n",
        "\n",
        "    create_repo(\n",
        "        repo_id=CONFIG[\"target_repo\"],\n",
        "        repo_type=\"dataset\",\n",
        "        private=True,\n",
        "        token=CONFIG[\"hf_token\"]\n",
        "    )\n",
        "\n",
        "    dsdict.push_to_hub(\n",
        "        repo_id=CONFIG[\"target_repo\"],\n",
        "        token=CONFIG[\"hf_token\"],\n",
        "        private=True,\n",
        "    )\n",
        "\n",
        "    with open(\"README.md\", \"w\") as f:\n",
        "        f.write(readme_fragment)\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=\"README.md\",\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=CONFIG[\"target_repo\"],\n",
        "        repo_type=\"dataset\",\n",
        "        token=CONFIG[\"hf_token\"]\n",
        "    )\n",
        "\n",
        "\n",
        "# ========= run & push =========\n",
        "def main():\n",
        "    raw_ds  = datasets.load_dataset(CONFIG[\"dataset_path\"])\n",
        "    split   = raw_ds[CONFIG[\"train_split\"]]\n",
        "\n",
        "    triplets = create_triplets(\n",
        "        split,\n",
        "        CONFIG[\"query_column\"],\n",
        "        CONFIG[\"positive_column\"],\n",
        "        max_tokens=CONFIG[\"max_tokens\"]\n",
        "    )\n",
        "\n",
        "    if not triplets:\n",
        "        raise RuntimeError(\"Triplets list empty!\")\n",
        "\n",
        "    train_ds = Dataset.from_list(triplets)\n",
        "    dsdict   = DatasetDict({\"train\": train_ds})\n",
        "    print(dsdict)\n",
        "\n",
        "    # ================== call the checker right before push ==================\n",
        "    stats = evaluate_hard_negative_quality(triplets)\n",
        "\n",
        "    readme_fragment = f\"\"\"\n",
        "    ### üîé Hard‚Äënegative sanity check\n",
        "    Randomly inspected {stats['sampled_triplets']:,} triplets with `deepvk/USER2-base`.\n",
        "\n",
        "    | metric | value |\n",
        "    | --- | --- |\n",
        "    | mean cos‚Äësim(query, **positive**) | **{stats['mean_pos_sim']:.4f}** |\n",
        "    | mean cos‚Äësim(query, negatives)   | {stats['mean_neg_sim']:.4f} |\n",
        "    | mean margin = pos ‚Äì hardest_neg  | {stats['mean_margin_pos_vs_hardest']:.4f} |\n",
        "    | bad cases (neg ‚â• pos)            | {stats['triplets_with_harder_negative']}/{stats['sampled_triplets']} |\n",
        "\n",
        "    Lower margin ‚áí harder negatives.\n",
        "    Ideally the last line should be 0.\n",
        "    \"\"\"\n",
        "\n",
        "    print(readme_fragment)\n",
        "    push_dataset(dsdict, readme_fragment)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-19T19:08:06.967045Z",
          "iopub.execute_input": "2025-04-19T19:08:06.967304Z",
          "execution_failed": "2025-04-19T19:10:08.557Z"
        },
        "id": "-4xjN2V-qlo6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TO5AR_WIqlo8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}